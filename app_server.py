from http.server import HTTPServer, SimpleHTTPRequestHandler
import json
import urllib.request
import urllib.parse
import re
import os
import sys
from database import init_db, save_cafes, get_cafes
import requests
from bs4 import BeautifulSoup

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "tr30Ch1tbJBqwNlv9svx")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "fsrn1wXmk3")

# ë©”ëª¨ë¦¬ ìºì‹œ (ë²„ì „ ì¶”ê°€ë¡œ ìºì‹œ ë¬´íš¨í™”)
blog_cache = {}
CACHE_VERSION = "v18"  # ìºì‹œ ë²„ì „ (ì¼ë°˜ ë‹¨ì–´ëŠ” ì§€ì—­ëª… í•„ìˆ˜)

def get_cafe_image_from_naver(cafe_name):
    """ë„¤ì´ë²„ ì´ë¯¸ì§€ ê²€ìƒ‰ APIë¡œ ì¹´í˜ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°"""
    url = "https://openapi.naver.com/v1/search/image"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {"query": cafe_name, "display": 1, "sort": "sim"}
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=2)
        if response.status_code == 200:
            data = response.json()
            if data['items']:
                return data['items'][0]['link']
    except:
        pass
    return None

def get_blog_image_url(blog_url):
    """ë¸”ë¡œê·¸ì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URLë§Œ ì¶”ì¶œ (ë‹¤ìš´ë¡œë“œ X)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://blog.naver.com/'
        }
        
        if 'm.blog.naver.com' in blog_url:
            blog_url = blog_url.replace('m.blog.naver.com', 'blog.naver.com')
        
        response = requests.get(blog_url, headers=headers, timeout=3)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        main_frame = soup.find('iframe', id='mainFrame')
        if not main_frame:
            return None
        
        actual_url = "https://blog.naver.com" + main_frame['src']
        res = requests.get(actual_url, headers=headers, timeout=3)
        content_soup = BeautifulSoup(res.text, 'html.parser')
        
        img_tags = content_soup.select('img[src*="postfiles.pstatic.net"]')
        if not img_tags:
            return None
        
        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ë°˜í™˜
        img = img_tags[0]
        img_url = img.get('data-lazy-src') or img.get('src')
        return img_url
    except:
        return None

def search_naver_blog(query, display=5):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ - test_server ì½”ë“œ ì‚¬ìš©"""
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {"query": query, "display": display * 2, "sort": "sim"}  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=3)
        if response.status_code == 200:
            data = response.json()
            return [item['link'] for item in data['items']]
    except:
        pass
    return []

def analyze_blog_content(cafe_name, cafe_address):
    # ìºì‹œ í™•ì¸ (ë²„ì „ í¬í•¨)
    cache_key = f"{CACHE_VERSION}_{cafe_name}_{cafe_address}"
    if cache_key in blog_cache:
        return blog_cache[cache_key]
    
    # ëŒ€í˜• ì¹´í˜ ë¸Œëœë“œ ë¦¬ìŠ¤íŠ¸
    major_brands = ['ëŒ€í˜•ì¹´í˜','ìŠ¤íƒ€ë²…ìŠ¤', 'íˆ¬ì¸í”Œë ˆì´ìŠ¤', 'íˆ¬ì¸', 'ì´ë””ì•¼', 'ì»¤í”¼ë¹ˆ', 'í• ë¦¬ìŠ¤', 'íƒì•¤íƒìŠ¤', 'íŒŒìŠ¤ì¿ ì°Œ', 'ì—”ì œë¦¬ë„ˆìŠ¤', 'ë””ì €íŠ¸39']
    is_major_cafe = any(brand in cafe_name for brand in major_brands)
    
    # íœ´ì–‘ì§€ ë¦¬ìŠ¤íŠ¸ (í–‰ì •êµ¬ì—­ í‚¤ì›Œë“œ)
    resort_areas = [
        # ê°•ì›ê¶Œ
        'ì–‘ì–‘êµ°', 'ì–‘ì–‘', 'ê°•ë¦‰ì‹œ', 'ê°•ë¦‰', 'ì†ì´ˆì‹œ', 'ì†ì´ˆ', 'ê³ ì„±êµ°', 'ê³ ì„±', 'ì‚¼ì²™ì‹œ', 'ì‚¼ì²™', 
        'í‰ì°½êµ°', 'í‰ì°½', 'ì •ì„ êµ°', 'ì •ì„ ', 'í™ì²œêµ°', 'í™ì²œ',
        # ì¸ì²œ/ê²½ê¸°ê¶Œ
        'ì¤‘êµ¬', 'ì›”ë¯¸ë„', 'ì˜ì¢…ë„', 'ê°•í™”êµ°', 'ê°•í™”', 'ì˜¹ì§„êµ°', 'ì˜¹ì§„', 'ê°€í‰êµ°', 'ê°€í‰', 
        'ì–‘í‰êµ°', 'ì–‘í‰', 'ëŒ€ë¶€ë„',
        # ì¶©ì²­ê¶Œ
        'íƒœì•ˆêµ°', 'íƒœì•ˆ', 'ì•ˆë©´ë„', 'ë³´ë ¹ì‹œ', 'ë³´ë ¹', 'ëŒ€ì²œ', 'ì„œì²œêµ°', 'ì„œì²œ', 'ë‹¨ì–‘êµ°', 'ë‹¨ì–‘',
        # ì „ë¼ê¶Œ
        'ì—¬ìˆ˜ì‹œ', 'ì—¬ìˆ˜', 'ìˆœì²œì‹œ', 'ìˆœì²œ', 'ì‹ ì•ˆêµ°', 'ì‹ ì•ˆ', 'ì§„ë„êµ°', 'ì§„ë„', 
        'ë¶€ì•ˆêµ°', 'ë¶€ì•ˆ', 'ì™„ë„êµ°', 'ì™„ë„',
        # ê²½ìƒê¶Œ
        'ê²½ì£¼ì‹œ', 'ê²½ì£¼', 'í¬í•­ì‹œ', 'í¬í•­', 'ê±°ì œì‹œ', 'ê±°ì œ', 'ë‚¨í•´êµ°', 'ë‚¨í•´', 
        'í†µì˜ì‹œ', 'í†µì˜', 'ìš¸ë¦‰êµ°', 'ìš¸ë¦‰ë„', 'ì˜ë•êµ°', 'ì˜ë•',
        # ì œì£¼ê¶Œ
        'ì œì£¼ì‹œ', 'ì œì£¼', 'ì„œê·€í¬ì‹œ', 'ì„œê·€í¬'
    ]
    is_resort_area = any(area in cafe_address for area in resort_areas)
    
    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API í˜¸ì¶œ
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    
    # ëŒ€í˜• ì¹´í˜ê°€ ì•„ë‹ˆë©´ "ì¹´ê³µ" í‚¤ì›Œë“œ ì¶”ê°€
    if is_major_cafe:
        query = f"{cafe_name} {cafe_address}"
    else:
        query = f"{cafe_name} {cafe_address} ì¹´ê³µ"
    
    params = {"query": query, "display": 100, "sort": "sim"}  # 30 â†’ 50ìœ¼ë¡œ ì¦ê°€
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=3)
        if response.status_code != 200:
            return get_empty_result()
            
        data = response.json()
        
        # ì¹´í˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ )
        # ì§€ì—­ëª… ì¶”ì¶œ (ì‹œ/êµ¬/ë™ ë‹¨ìœ„)
        address_parts = cafe_address.split()
        location_keyword = ""
        for part in address_parts:
            if 'ì‹œ' in part or 'êµ¬' in part or 'ë™' in part or 'ì' in part or 'ë©´' in part:
                location_keyword = part.replace('ì‹œ', '').replace('êµ¬', '').replace('ë™', '').replace('ì', '').replace('ë©´', '')
                break
        
        if cafe_name.endswith('ì '):
            # "ìŠ¤íƒ€ë²…ìŠ¤ ê°•ë‚¨ì " â†’ "ìŠ¤íƒ€ë²…ìŠ¤ ê°•ë‚¨"
            cafe_keyword = cafe_name.replace('ì¹´í˜', '').replace('ì»¤í”¼', '').strip()
        else:
            # ì¼ë°˜ ì¹´í˜ëª… ì²˜ë¦¬
            temp_name = cafe_name.replace('ì¹´í˜', '').replace('ì»¤í”¼', '').replace('ì ', '').strip()
            
            # ìˆ«ìë‚˜ "24ì‹œ", "ë¬´ì¸" ê°™ì€ ì¼ë°˜ ë‹¨ì–´ ì œê±°
            words = temp_name.split()
            meaningful_words = []
            skip_words = ['24ì‹œ', 'ë¬´ì¸', 'ì…€í”„', 'ìŠ¤í„°ë””', 'ê³µë¶€ë°©', 'ë…ì„œì‹¤']
            
            for word in words:
                # ìˆ«ìë¡œë§Œ êµ¬ì„±ë˜ê±°ë‚˜ skip_wordsì— ìˆìœ¼ë©´ ì œì™¸
                if not word.isdigit() and word not in skip_words:
                    meaningful_words.append(word)
            
            # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            if meaningful_words:
                cafe_keyword = ' '.join(meaningful_words)
            else:
                cafe_keyword = temp_name
        
        # ì¼ë°˜ì ì¸ ë‹¨ì–´(2ê¸€ì ì´í•˜ ë˜ëŠ” í”í•œ ë‹¨ì–´)ëŠ” ì§€ì—­ëª… í•„ìˆ˜
        common_words = ['ì—¬ìœ ', 'íë§', 'ì‰¼', 'íœ´ì‹', 'í–‰ë³µ', 'ì‚¬ë‘', 'í‰í™”', 'ì˜¨', 'ìˆ²', 'ë°”ë‹¤', 'í•˜ëŠ˜']
        needs_location = len(cafe_keyword) <= 2 or any(word in cafe_keyword for word in common_words)
        
        print(f"ğŸ” ì¹´í˜ëª…: {cafe_name} â†’ ê²€ìƒ‰ í‚¤ì›Œë“œ: {cafe_keyword}, ì§€ì—­: {location_keyword}, ì§€ì—­í•„ìˆ˜: {needs_location}")
        
        # í•„í„°ë§ ë° í‚¤ì›Œë“œ ë¶„ì„
        filtered_urls = []
        filtered_items = []  # ì œëª©ê³¼ ì„¤ëª…ë„ í•¨ê»˜ ì €ì¥
        all_text = ""
        cafe_description = ""
        
        # 1ì°¨ í•„í„°ë§: ì‘ì—… í‚¤ì›Œë“œ í¬í•¨
        work_filtered_items = []
        basic_filtered_items = []  # ì¹´í˜ëª…+ì§€ì—­ë§Œ í¬í•¨
        
        for item in data['items']:
            title = item.get('title', '').replace('<b>', '').replace('</b>', '')
            description = item.get('description', '').replace('<b>', '').replace('</b>', '')
            combined = (title + ' ' + description).lower()
            
            # ì¹´í˜ ì´ë¦„ í¬í•¨ í™•ì¸
            if cafe_keyword.lower() not in combined:
                continue
            
            # ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” ì§€ì—­ëª…ë„ í•„ìˆ˜
            if needs_location and location_keyword:
                if location_keyword.lower() not in combined:
                    continue
            
            # ì¹´í˜/ì»¤í”¼ í‚¤ì›Œë“œ í•„ìˆ˜ (ì¹´í˜ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì¥ì†Œ ì œì™¸)
            if 'ì¹´í˜' not in combined and 'ì»¤í”¼' not in combined and 'cafe' not in combined and 'coffee' not in combined:
                continue
            
            item_data = {
                'link': item['link'],
                'title': title,
                'description': description,
                'combined': combined
            }
            
            # ì‘ì—… í‚¤ì›Œë“œ ì²´í¬
            work_keywords = ['ì¹´ê³µ', 'ê³µë¶€', 'ì‘ì—…', 'ë…¸íŠ¸ë¶', 'ì¡°ìš©', 'ì§‘ì¤‘', 'ë„“ì€', 'ì¢Œì„', 'ì±…', 'ì™€ì´íŒŒì´', 'wifi', 'ì½˜ì„¼íŠ¸', 'ì¶©ì „', 'ìŠ¤í„°ë””']
            has_work_keyword = any(keyword in combined for keyword in work_keywords)
            
            if has_work_keyword:
                work_filtered_items.append(item_data)
            else:
                basic_filtered_items.append(item_data)
        
        # 2ì°¨ í•„í„°ë§: ì‘ì—… í‚¤ì›Œë“œ ìˆëŠ” ê²ƒ ìš°ì„ , ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ í•„í„°ë§ ì¶”ê°€
        # íœ´ì–‘ì§€ë‚˜ ëŒ€í˜• ì¹´í˜ëŠ” ì‘ì—… í‚¤ì›Œë“œ ì—†ì–´ë„ OK
        if is_major_cafe or is_resort_area:
            final_items = work_filtered_items + basic_filtered_items
        else:
            # ì‘ì—… í‚¤ì›Œë“œ ìˆëŠ” ê²ƒì´ 5ê°œ ë¯¸ë§Œì´ë©´ ê¸°ë³¸ í•„í„°ë§ë„ ì¶”ê°€ (ìµœëŒ€ 20ê°œ)
            if len(work_filtered_items) < 5:
                final_items = work_filtered_items + basic_filtered_items[:20]
            else:
                final_items = work_filtered_items
        
        # ìµœì¢… ê²°ê³¼ ìƒì„± (ìµœëŒ€ 20ê°œ)
        for item_data in final_items[:20]:
            filtered_urls.append(item_data['link'])
            filtered_items.append({
                'url': item_data['link'],
                'title': item_data['title'],
                'description': item_data['description'][:100] + '...' if len(item_data['description']) > 100 else item_data['description']
            })
            all_text += " " + item_data['title'] + " " + item_data['description']
            if not cafe_description:
                cafe_description = item_data['description'][:80] + "..." if len(item_data['description']) > 80 else item_data['description']
        
        if not filtered_urls:
            return get_empty_result()
        
        # í‚¤ì›Œë“œ ì¹´ìš´íŒ…
        text_lower = all_text.lower()
        
        # ì½˜ì„¼íŠ¸ ì ìœ ìœ¨ (í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ íŒë‹¨)
        outlet_count = text_lower.count('ì½˜ì„¼íŠ¸') + text_lower.count('ì¶©ì „') + text_lower.count('í”ŒëŸ¬ê·¸')
        
        # "ì½˜ì„¼íŠ¸ ë§ì•„ìš”", "ì½˜ì„¼íŠ¸ ë„‰ë„‰", "ëª¨ë“  ì¢Œì„ ì½˜ì„¼íŠ¸" ë“±
        if ('ì½˜ì„¼íŠ¸' in text_lower or 'ì¶©ì „' in text_lower) and \
           ('ë§' in text_lower or 'ë„‰ë„‰' in text_lower or 'ëª¨ë“ ' in text_lower or 'ì „ë¶€' in text_lower or 'ì¶©ë¶„' in text_lower):
            outlet_level = "ëª¨ë“  ì¢Œì„"
        elif ('ì½˜ì„¼íŠ¸' in text_lower or 'ì¶©ì „' in text_lower) and \
             ('ë°˜' in text_lower or 'ì ˆë°˜' in text_lower or 'ì¼ë¶€' in text_lower):
            outlet_level = "50% ì •ë„"
        elif outlet_count >= 1:
            outlet_level = "ë²½ë©´ì—ë§Œ"
        else:
            outlet_level = "ì •ë³´ ì—†ìŒ"
        
        # ì†ŒìŒ ë ˆë²¨
        quiet_words = text_lower.count('ì¡°ìš©') + text_lower.count('ì§‘ì¤‘') + text_lower.count('ë…ì„œì‹¤') + text_lower.count('ì°¨ë¶„')
        noisy_words = text_lower.count('ì‹œë„') + text_lower.count('ë– ë“¤') + text_lower.count('ë¶ì ') + text_lower.count('ì‹œëŒ')
        if quiet_words >= 5:
            noise_level = "ë…ì„œì‹¤ ìˆ˜ì¤€"
        elif quiet_words >= 2:
            noise_level = "ì”ì”í•œ ìŒì•…"
        elif noisy_words >= 3:
            noise_level = "ëŒ€í™” í™œë°œ"
        else:
            noise_level = "ë³´í†µ"
        
        # ì‘ì—… ì í•©ë„
        work_mentions = (text_lower.count('ë…¸íŠ¸ë¶') + text_lower.count('ì‘ì—…') + 
                        text_lower.count('ê³µë¶€') + text_lower.count('ì¹´ê³µ') + 
                        text_lower.count('ìŠ¤í„°ë””') + text_lower.count('ì—…ë¬´'))
        
        work_positive = (text_lower.count('ì‘ì—…í•˜ê¸° ì¢‹') + text_lower.count('ê³µë¶€í•˜ê¸° ì¢‹') + 
                        text_lower.count('ì¹´ê³µí•˜ê¸° ì¢‹') + text_lower.count('ë…¸íŠ¸ë¶ í•˜ê¸° ì¢‹') +
                        text_lower.count('ì‘ì—… ì¶”ì²œ') + text_lower.count('ê³µë¶€ ì¶”ì²œ') +
                        text_lower.count('ì¹´ê³µ ì¶”ì²œ') + text_lower.count('ì¹´ê³µ ì¢‹'))
        
        work_negative = (text_lower.count('ì‘ì—…í•˜ê¸° ì•ˆì¢‹') + text_lower.count('ì‘ì—…í•˜ê¸° ì•ˆ ì¢‹') +
                        text_lower.count('ê³µë¶€í•˜ê¸° ì•ˆì¢‹') + text_lower.count('ê³µë¶€í•˜ê¸° ì•ˆ ì¢‹') +
                        text_lower.count('ì¹´ê³µ ë¹„ì¶”') + text_lower.count('ì¹´ê³µ ì•ˆì¢‹'))
        
        work_score = 0
        if work_positive > 0:
            work_score = 8 + (work_mentions * 0.5)
        elif work_mentions > 0:
            work_score = work_mentions * 0.5
        work_score -= work_negative * 1
        work_score = max(0, min(10, work_score))  # 0~10ì  ì œí•œ
        
        # ê³µê°„ê° (í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ íŒë‹¨)
        space_words = text_lower.count('ë„“ì€') + text_lower.count('ë„“ì–´') + text_lower.count('ì—¬ìœ ') + text_lower.count('ì¾Œì ') + text_lower.count('ê³µê°„')
        cramped_words = text_lower.count('ì¢ì€') + text_lower.count('ì¢ì•„') + text_lower.count('ë¹„ì¢')
        
        # ëŒ€í˜• ì¹´í˜ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë„“ì€ í¸
        if is_major_cafe:
            if cramped_words >= 1:
                space_level = "ì¢ì€ í¸"
            else:
                space_level = "ë„“ì€ í¸"  # ëŒ€í˜• ì¹´í˜ ê¸°ë³¸ê°’
        # "ë„“ì€ ê³µê°„", "ê³µê°„ì´ ë„“ì–´ìš”", "ì—¬ìœ ë¡œìš´ ì¢Œì„" ë“±
        elif ('ë„“' in text_lower or 'ì—¬ìœ ' in text_lower or 'ì¾Œì ' in text_lower) and \
           ('ê³µê°„' in text_lower or 'ì¢Œì„' in text_lower or 'ë§¤ì¥' in text_lower):
            space_level = "ë§¤ìš° ë„“ìŒ"
        elif space_words >= 1:
            space_level = "ë„“ì€ í¸"
        elif cramped_words >= 1:
            space_level = "ì¢ì€ í¸"
        else:
            space_level = "ì •ë³´ ì—†ìŒ"
        
        # í…Œì´ë¸” ë†’ì´
        if 'ë†’' in text_lower and 'í…Œì´ë¸”' in text_lower:
            table_height = "ë…¸íŠ¸ë¶ í•˜ê¸° ì¢‹ìŒ"
        elif 'ë‚®' in text_lower and 'í…Œì´ë¸”' in text_lower:
            table_height = "ì¸ìŠ¤íƒ€ ê°ì„±í˜•"
        else:
            table_height = "ì •ë³´ ì—†ìŒ"
        
        # ì´ìš© ì œí•œ
        if 'ì‹œê°„ì œí•œ' in text_lower or 'ì‹œê°„ ì œí•œ' in text_lower:
            time_limit = "ì‹œê°„ ì œí•œ ìˆìŒ"
        elif 'ì¹´ê³µ' in text_lower and ('í™˜ì˜' in text_lower or 'ì¶”ì²œ' in text_lower):
            time_limit = "ì¹´ê³µ í™˜ì˜"
        else:
            time_limit = "ì •ë³´ ì—†ìŒ"
        
        # ì™€ì´íŒŒì´
        wifi_count = text_lower.count('ì™€ì´íŒŒì´') + text_lower.count('wifi') + text_lower.count('ì¸í„°ë„·')
        has_wifi = wifi_count > 0
        
        # ì£¼ì°¨
        parking_count = text_lower.count('ì£¼ì°¨')
        has_parking = parking_count > 0
        
        # ì‹ í˜¸ë“± ìƒ‰ìƒ (ì¢…í•© ì ìˆ˜ ê¸°ë°˜)
        # ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ íšŒìƒ‰ (ëŒ€í˜• ì¹´í˜ ì œì™¸)
        if len(filtered_urls) == 0:
            if is_major_cafe:
                signal_color = "yellow"  # ëŒ€í˜• ì¹´í˜ëŠ” ê¸°ë³¸ ë…¸ë€ìƒ‰
            else:
                signal_color = "gray"  # ë¦¬ë·° ì—†ìŒ
        else:
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 5ì )
            total_score = 0
            review_count = len(filtered_urls)
            
            # 1. ì‘ì—… ì í•©ë„ (ìµœëŒ€ 2.8ì )
            if work_score >= 10:
                total_score += 2.8
            elif work_score >= 8:
                total_score += 2.2
            elif work_score >= 5:
                total_score += 1.5
            elif work_score >= 2:
                total_score += 0.8
            
            # 2. ì½˜ì„¼íŠ¸ (ìµœëŒ€ 0.4ì )
            if outlet_level == "ëª¨ë“  ì¢Œì„":
                total_score += 0.4
            elif outlet_level == "50% ì •ë„":
                total_score += 0.28
            elif outlet_level == "ë²½ë©´ì—ë§Œ":
                total_score += 0.2
            
            # 3. ì†ŒìŒ ë ˆë²¨ (ìµœëŒ€ 0.3ì )
            if noise_level == "ë…ì„œì‹¤ ìˆ˜ì¤€":
                total_score += 0.3
            elif noise_level == "ì”ì”í•œ ìŒì•…":
                total_score += 0.21
            elif noise_level == "ë³´í†µ":
                total_score += 0.15
            
            # 4. ê³µê°„ê° (ìµœëŒ€ 0.8ì )
            if space_level == "ë§¤ìš° ë„“ìŒ":
                total_score += 0.8
            elif space_level == "ë„“ì€ í¸":
                total_score += 0.5
            
            # 5. WiFi (ìµœëŒ€ 0.4ì )
            if has_wifi:
                total_score += 0.4
            
            # 6. ë¦¬ë·° ê°œìˆ˜ (ìµœëŒ€ 0.3ì )
            if review_count >= 15:
                total_score += 0.3
            elif review_count >= 10:
                total_score += 0.23
            elif review_count >= 5:
                total_score += 0.15
            
            # ëŒ€í˜• ì¹´í˜ëŠ” ìµœì†Œ 2.5ì  ë³´ì¥
            if is_major_cafe and total_score < 2.5:
                total_score = 2.5
            
            # íœ´ì–‘ì§€ ë³´ë„ˆìŠ¤ +1ì 
            if is_resort_area:
                total_score += 1
            
            # ëŒ€í˜• ì¹´í˜ ë³´ë„ˆìŠ¤ +1ì 
            if is_major_cafe:
                total_score += 1
            
            # ìµœëŒ€ 5ì  ì œí•œ
            total_score = min(5.0, total_score)
            
            # ì‹ í˜¸ë“± ìƒ‰ìƒ ê²°ì •
            if total_score >= 3.7:
                signal_color = "green"
            elif total_score >= 2.5:
                signal_color = "yellow"
            else:
                signal_color = "red"
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
        keywords = {}
        keyword_list = ['ë…¸íŠ¸ë¶', 'ì‘ì—…', 'ê³µë¶€', 'ì¹´ê³µ', 'ì¡°ìš©', 'ì§‘ì¤‘', 'ë„“ì€', 'ì¢Œì„', 'ì½˜ì„¼íŠ¸', 'ì¶©ì „', 'ì™€ì´íŒŒì´', 'wifi']
        for keyword in keyword_list:
            count = text_lower.count(keyword)
            if count > 0:
                keywords[keyword] = count
        
        result = {
            "workScore": round(work_score, 1),
            "outletLevel": outlet_level,
            "noiseLevel": noise_level,
            "spaceLevel": space_level,
            "tableHeight": table_height,
            "timeLimit": time_limit,
            "hasWifi": has_wifi,
            "hasParking": has_parking,
            "signalColor": signal_color,
            "blogCount": len(filtered_urls),
            "blogUrls": filtered_urls,
            "blogItems": filtered_items,
            "description": cafe_description,
            "keywords": keywords,
            "totalScore": round(total_score, 1) if len(filtered_urls) > 0 else 0
        }
        
        blog_cache[cache_key] = result
        return result
        
    except Exception as e:
        print(f"Error analyzing blog: {e}")
        return get_empty_result()

def get_empty_result():
    return {
        "workScore": 0,
        "outletLevel": "ì •ë³´ ì—†ìŒ",
        "noiseLevel": "ì •ë³´ ì—†ìŒ",
        "spaceLevel": "ì •ë³´ ì—†ìŒ",
        "tableHeight": "ì •ë³´ ì—†ìŒ",
        "timeLimit": "ì •ë³´ ì—†ìŒ",
        "hasWifi": False,
        "hasParking": False,
        "signalColor": "gray",
        "blogCount": 0,
        "blogUrls": [],
        "blogItems": [],
        "description": "",
        "keywords": {},
        "totalScore": 0
    }

class Handler(SimpleHTTPRequestHandler):
    def do_OPTIONS(self):
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()

    def do_POST(self):
        if self.path == '/api/blog-search':
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode('utf-8'))
            
            cafe_name = data.get('name', '')
            cafe_address = data.get('address', '')
            
            result = analyze_blog_content(cafe_name, cafe_address)
            
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps(result, ensure_ascii=False).encode('utf-8'))
        elif self.path == '/api/clear-cache':
            global blog_cache
            blog_cache.clear()
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps({"status": "ok", "message": "Cache cleared"}).encode('utf-8'))
        else:
            self.send_error(404)

    def do_GET(self):
        if self.path == '/':
            self.path = '/index.html'
        return SimpleHTTPRequestHandler.do_GET(self)

if __name__ == '__main__':
    print("ğŸš€ Venue app running at http://localhost:5000")
    print("ğŸ“Š Database: venue.db")
    print("ğŸ”„ Refresh data: http://localhost:5000/api/refresh")
    HTTPServer(('', 5000), Handler).serve_forever()
